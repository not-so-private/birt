{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Parse IRT dataset from jsonlines, formatted in the following way:\n",
    "* The dataset is in jsonlines format, each line representing the responses of a subject\n",
    "* Each row looks like this:\n",
    "{\"subject_id\": \"<subject_id>\", \"responses\": {\"<item_id>\": <response>}}\n",
    "* Where <subject_id> is a string, <item_id> is a string, and <response> is a number (usually integer)\n",
    "```  \n",
    "- mengubah value table dengan perkiraan ini  \n",
    "![](images/estimasi_level.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olah data\n",
    "## Download table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor modul requests untuk melakukan HTTP requests\n",
    "import requests\n",
    "\n",
    "# Daftar sumber data JSON dan URL-nya\n",
    "sources = {\n",
    "    \"Satellite\": \"https://stellabms.xyz/sl/score.json\",\n",
    "    \"Stella\": \"https://stellabms.xyz/st/score.json\",\n",
    "    \"Insane1\": \"http://www.ribbit.xyz/bms/tables/insane_body.json\",\n",
    "    \"Insane2\": \"https://rattoto10.github.io/second_table/insane_data.json\",\n",
    "}\n",
    "\n",
    "# Lokasi penyimpanan file output untuk masing-masing sumber\n",
    "output_paths = {\n",
    "    \"Satellite\": \"dataset/table_ori/satellite.json\",\n",
    "    \"Stella\": \"dataset/table_ori/stella.json\",\n",
    "    \"Insane1\": \"dataset/table_ori/insane1.json\",\n",
    "    \"Insane2\": \"dataset/table_ori/insane2.json\",\n",
    "}\n",
    "\n",
    "# Loop utama untuk mengunduh data dari semua sumber\n",
    "for source_name, source_url in sources.items():\n",
    "    # Mengirim GET request ke URL sumber\n",
    "    response = requests.get(source_url)\n",
    "\n",
    "    # Memeriksa status code response\n",
    "    if response.status_code == 200:\n",
    "        # Mengatur encoding response ke UTF-8 untuk handle karakter khusus\n",
    "        response.encoding = 'utf-8'\n",
    "        \n",
    "        # Parse data JSON dari response (tidak digunakan tapi bisa untuk validasi)\n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Mengambil path output yang sesuai dari dictionary output_paths\n",
    "        output_path = output_paths[source_name]\n",
    "        \n",
    "        # Menyimpan data ke file dengan encoding UTF-8\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        \n",
    "        # Notifikasi sukses dengan nama sumber dan path output\n",
    "        print(f\"Downloaded and saved {source_name} JSON data to {output_path}\")\n",
    "    else:\n",
    "        # Notifikasi gagal dengan informasi status code\n",
    "        print(f\"Failed to retrieve {source_name} JSON data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ganti level table dan gabungkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Daftar path file dataset yang akan diproses\n",
    "dataset_paths = [\n",
    "    \"dataset/table_ori/insane1.json\",\n",
    "    \"dataset/table_ori/insane2.json\",\n",
    "    \"dataset/table_ori/satellite.json\",\n",
    "    \"dataset/table_ori/stella.json\"\n",
    "]\n",
    "\n",
    "# Dictionary untuk menyimpan data gabungan dengan MD5 sebagai key\n",
    "combined_data = {}\n",
    "\n",
    "def transform_level(dataset_name, level):\n",
    "    ###### Fungsi untuk transformasi level sesuai kriteria khusus tiap dataset\n",
    "    # Konversi level ke integer jika berupa digit\n",
    "    if level.isdigit():\n",
    "        level = int(level)\n",
    "    else:\n",
    "        level = 0  # Default untuk nilai non-numerik\n",
    "    \n",
    "    # Logika transformasi berdasarkan nama dataset\n",
    "    if dataset_name == \"insane1.json\":\n",
    "        return level + 11\n",
    "    elif dataset_name == \"insane2.json\":\n",
    "        # Handle kasus khusus level 0- dan 0\n",
    "        if level == \"0-\":\n",
    "            return 11.5\n",
    "        elif level == \"0\":\n",
    "            return 11.8\n",
    "        else:\n",
    "            return int(level) + 11\n",
    "    elif dataset_name == \"satellite.json\":\n",
    "        # Mapping level menggunakan array khusus\n",
    "        return [0.5,1.5,3,4.5,6.5,8.5,10.5,12,13.5,15.5,16.5,17.5,19][level] + 11\n",
    "    elif dataset_name == \"stella.json\":\n",
    "        # Mapping level dengan array berbeda\n",
    "        return [19.5,21,22,22.5,23.5,24,24.25,24.5,24.75,25,25.5,26,27,27.5][level] + 11\n",
    "\n",
    "# Proses file dataset secara terbalik (prioritas dataset terakhir)\n",
    "for dataset_path in reversed(dataset_paths):\n",
    "    # Baca file JSON\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        dataset = json.load(file)\n",
    "    \n",
    "    # Proses setiap item dalam dataset\n",
    "    for item in dataset:\n",
    "        md5 = item[\"md5\"]\n",
    "        level = item[\"level\"]\n",
    "        \n",
    "        # Transformasi level dan update nilai\n",
    "        item[\"level\"] = transform_level(dataset_path.split(\"/\")[-1], level)\n",
    "        \n",
    "        # Update data dengan MD5 sebagai key (data terakhir menimpa yang lama)\n",
    "        combined_data[md5] = item\n",
    "\n",
    "# Konversi dictionary ke list untuk output\n",
    "combined_data_list = list(combined_data.values())\n",
    "\n",
    "# Simpan data gabungan ke file JSON\n",
    "with open(\"dataset/combined_dataset.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(combined_data_list, outfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dapatkan player score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "def fetch_scores_for_md5(md5, cache_dir):\n",
    "    ### Mengambil data skor dari cache atau API eksternal dengan mekanisme fallback\n",
    "    # Membentuk path file cache berdasarkan MD5\n",
    "    cache_file = os.path.join(cache_dir, f'{md5}.json')\n",
    "\n",
    "    try:\n",
    "        # Coba baca dari cache\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Validasi format cache\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(f'Invalid cache for {md5}')\n",
    "\n",
    "        return data\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        # Fallback ke API jika cache tidak ada/invalid\n",
    "        response = requests.get(f'http://dream-pro.info/~lavalse/LR2IR/2/getrankingxml.cgi?songmd5={md5}&id=1')\n",
    "        response_text = response.text\n",
    "\n",
    "        # Ekstrak bagian XML dari response\n",
    "        xml_start = response_text.find('<?xml')\n",
    "        xml_end = response_text.rfind('</ranking>') + len('</ranking>')\n",
    "        if xml_start != -1 and xml_end != -1:\n",
    "            xml_text = response_text[xml_start:xml_end]\n",
    "        else:\n",
    "            raise ValueError(f'Invalid XML response for {md5}')\n",
    "\n",
    "        # Parsing data XML\n",
    "        xml_root = ET.fromstring(xml_text)\n",
    "        data = []\n",
    "\n",
    "        # Ekstrak data skor dari XML\n",
    "        for score_elem in xml_root.findall('.//score'):\n",
    "            score_data = {\n",
    "                'name': score_elem.find('name').text,\n",
    "                'id': int(score_elem.find('id').text),\n",
    "                'clear': int(score_elem.find('clear').text),\n",
    "                'notes': int(score_elem.find('notes').text),\n",
    "                'combo': int(score_elem.find('combo').text),\n",
    "                'pg': int(score_elem.find('pg').text),\n",
    "                'gr': int(score_elem.find('gr').text),\n",
    "                'minbp': int(score_elem.find('minbp').text),\n",
    "            }\n",
    "            data.append(score_data)\n",
    "\n",
    "        # Konversi nama ke string\n",
    "        for d in data:\n",
    "            d['name'] = str(d['name'])\n",
    "\n",
    "        # Simpan ke cache untuk penggunaan berikutnya\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent='\\t')\n",
    "\n",
    "        return data\n",
    "\n",
    "def get_scores_for_md5(md5, cache_dir):\n",
    "    ### Wrapper dengan mekanisme retry untuk handle throttling\n",
    "    tries = 0\n",
    "\n",
    "    # Maksimal 3 kali percobaan\n",
    "    while tries < 3:\n",
    "        try:\n",
    "            data = fetch_scores_for_md5(md5, cache_dir)\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            tries += 1\n",
    "            # Kalkulasi waktu tunggu eksponensial\n",
    "            sleep_time = (1000 * tries + (0.5 - 1) * 1000) * 2\n",
    "            print(f'Got throttled ({md5}): Sleeping for {sleep_time:.0f}ms. {err}')\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    raise Exception(f\"Couldn't fetch data in 3 tries. Giving up and exiting.\")\n",
    "\n",
    "# Konfigurasi direktori cache\n",
    "cache_directory = \"dataset/lr2ir\" \n",
    "\n",
    "# Persiapan daftar MD5 dari dataset gabungan\n",
    "md5_values = []\n",
    "\n",
    "# Baca file dataset kombinasi\n",
    "json_files = [\"dataset/combined_dataset.json\"]\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        # Kumpulkan semua nilai MD5\n",
    "        for item in data:\n",
    "            md5_values.append(item['md5'])\n",
    "\n",
    "# Proses setiap MD5 untuk mendapatkan skor\n",
    "for md5 in md5_values:\n",
    "    scores = get_scores_for_md5(md5, cache_directory)\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataset dengan kondisi clear > 0 and minbp < notes * 0.2\n",
    "0 = no play  \n",
    "1 = failed  \n",
    "2 = easy clear  \n",
    "3 = normal clear  \n",
    "4 = hard clear  \n",
    "5 = full combo  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Membaca dataset gabungan sebagai referensi\n",
    "with open(\"dataset/combined_dataset.json\", \"r\", encoding=\"utf-8\") as combined_file:\n",
    "    combined_data = json.load(combined_file)\n",
    "\n",
    "def filter_and_save_dataset(md5):\n",
    "    ### Memfilter dan menyimpan dataset berdasarkan kriteria performa bermain\n",
    "    # Path file dataset asli dan hasil filter\n",
    "    dataset_path = f\"dataset/lr2ir/{md5}.json\"\n",
    "    \n",
    "    # Cek keberadaan file dataset\n",
    "    if os.path.exists(dataset_path):\n",
    "        # Baca dataset skor mentah\n",
    "        with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            dataset = json.load(file)\n",
    "        \n",
    "        # Filter data: clear > 0 dan minbp < 20% notes\n",
    "        filtered_dataset = [item for item in dataset \n",
    "                           if item[\"clear\"] > 0 \n",
    "                           and item[\"minbp\"] < item[\"notes\"] * 0.2]\n",
    "        \n",
    "        # Simpan dataset terfilter\n",
    "        filtered_path = f\"dataset/filtered_lr2ir/{md5}.json\"\n",
    "        with open(filtered_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            json.dump(filtered_dataset, outfile, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        # Handle kasus file tidak ditemukan\n",
    "        print(f\"Dataset not found for MD5: {md5}\")\n",
    "\n",
    "# Proses filter untuk semua entry dalam combined_data\n",
    "for item in combined_data:\n",
    "    md5 = item[\"md5\"]\n",
    "    filter_and_save_dataset(md5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akuisisi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Membaca dataset gabungan sebagai referensi utama\n",
    "with open(\"dataset/combined_dataset.json\", \"r\", encoding=\"utf-8\") as combined_file:\n",
    "    combined_data = json.load(combined_file)\n",
    "\n",
    "def process_subject_responses(md5):\n",
    "    ### Membuat dataset mudah dengan kriteria clear >= 2 (easy clear)\n",
    "    # Struktur dasar untuk menyimpan respons pemain\n",
    "    response_dict = {\"subject_id\": md5, \"responses\": {}}\n",
    "    \n",
    "    # Path dataset yang sudah difilter\n",
    "    dataset_path = f\"dataset/filtered_lr2ir/{md5}.json\"\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            dataset = json.load(file)\n",
    "\n",
    "        # Pemrosesan setiap skor pemain\n",
    "        for item in dataset:\n",
    "            item_id = str(item[\"id\"])\n",
    "            clear = item[\"clear\"]\n",
    "            # Kriteria respon: 1 jika clear status >= 2\n",
    "            response = 1 if clear >= 2 else 0\n",
    "            response_dict[\"responses\"][item_id] = response\n",
    "            \n",
    "        # Menyimpan ke format JSON Lines dengan append mode\n",
    "        with open(\"dataset/easy_dataset.jsonlines\", \"a\", encoding=\"utf-8\") as outfile:\n",
    "            json.dump(response_dict, outfile, ensure_ascii=False)\n",
    "            outfile.write(\"\\n\")\n",
    "    else:\n",
    "        print(f\"Filtered dataset not found for MD5: {md5}\")\n",
    "\n",
    "# Eksekusi untuk semua MD5 dalam dataset gabungan\n",
    "for item in combined_data:\n",
    "    md5 = item[\"md5\"]\n",
    "    process_subject_responses(md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Membaca dataset gabungan sebagai referensi utama\n",
    "with open(\"dataset/combined_dataset.json\", \"r\", encoding=\"utf-8\") as combined_file:\n",
    "    combined_data = json.load(combined_file)\n",
    "\n",
    "def process_subject_responses(md5):\n",
    "    ### Membuat dataset mudah dengan kriteria clear >= 4 (hard clear)\n",
    "    # Struktur dasar untuk menyimpan respons pemain\n",
    "    response_dict = {\"subject_id\": md5, \"responses\": {}}\n",
    "\n",
    "    dataset_path = f\"dataset/filtered_lr2ir/{md5}.json\"\n",
    "    if os.path.exists(dataset_path):\n",
    "        with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            dataset = json.load(file)\n",
    "\n",
    "        for item in dataset:\n",
    "            item_id = str(item[\"id\"])\n",
    "            clear = item[\"clear\"]\n",
    "            # Kriteria respon: 1 jika clear status >= 4\n",
    "            response = 1 if clear >= 4 else 0\n",
    "            response_dict[\"responses\"][item_id] = response\n",
    "\n",
    "        # Menyimpan ke format JSON Lines dengan append mode\n",
    "        with open(\"dataset/hard_dataset.jsonlines\", \"a\", encoding=\"utf-8\") as outfile:\n",
    "            json.dump(response_dict, outfile, ensure_ascii=False)\n",
    "            outfile.write(\"\\n\")\n",
    "    else:\n",
    "        print(f\"Filtered dataset not found for MD5: {md5}\")\n",
    "\n",
    "# Eksekusi untuk semua MD5 dalam dataset gabungan\n",
    "for item in combined_data:\n",
    "    md5 = item[\"md5\"]\n",
    "    process_subject_responses(md5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run py-irt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train dan evaluasi data\n",
    "By default this will train a model with 90% of the provided data and evaluate with the remaining 10% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      " Usage: py-irt.cmd train-and-evaluate [OPTIONS] MODEL_TYPE DATA_PATH           \n",
      " OUTPUT_DIR                                                                    \n",
      "                                                                               \n",
      "┌─ Arguments ─────────────────────────────────────────────────────────────────┐\n",
      "│ *    model_type      TEXT  [default: None] [required]                       │\n",
      "│ *    data_path       TEXT  [default: None] [required]                       │\n",
      "│ *    output_dir      TEXT  [default: None] [required]                       │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "┌─ Options ───────────────────────────────────────────────────────────────────┐\n",
      "│ --epochs              INTEGER  [default: 2000]                              │\n",
      "│ --priors              TEXT     [default: None]                              │\n",
      "│ --dims                INTEGER  [default: None]                              │\n",
      "│ --device              TEXT     [default: cpu]                               │\n",
      "│ --lr                  FLOAT    [default: None]                              │\n",
      "│ --lr-decay            FLOAT    [default: None]                              │\n",
      "│ --initializers        TEXT     [default: None]                              │\n",
      "│ --evaluation          TEXT     [default: heldout]                           │\n",
      "│ --seed                INTEGER  [default: 42]                                │\n",
      "│ --train-size          FLOAT    [default: 0.9]                               │\n",
      "│ --config-path         TEXT     [default: None]                              │\n",
      "│ --dropout             FLOAT    [default: 0.5]                               │\n",
      "│ --hidden              INTEGER  [default: 100]                               │\n",
      "│ --help                         Show this message and exit.                  │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!poetry run py-irt train-and-evaluate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: py-irt.cmd evaluate [OPTIONS] MODEL_TYPE PARAMETER_PATH TEST_PAIRS_PATH\n",
      "                           OUTPUT_DIR\n",
      "\n",
      "Arguments:\n",
      "  MODEL_TYPE       [required]\n",
      "  PARAMETER_PATH   [required]\n",
      "  TEST_PAIRS_PATH  [required]\n",
      "  OUTPUT_DIR       [required]\n",
      "\n",
      "Options:\n",
      "  --epochs INTEGER     [default: 2000]\n",
      "  --device TEXT        [default: cpu]\n",
      "  --initializers TEXT\n",
      "  --evaluation TEXT    [default: heldout]\n",
      "  --seed INTEGER       [default: 42]\n",
      "  --train-size FLOAT   [default: 0.9]\n",
      "  --help               Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!poetry run py-irt evaluate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:45:41] config: model_type='2pl' epochs=2000 priors=None          cli.py:176\n",
      "           initializers=[] dims=None lr=0.1 lr_decay=0.9999                    \n",
      "           dropout=0.5 hidden=100 vocab_size=None log_every=100                \n",
      "           seed=None deterministic=False                                       \n",
      "           data_path: dataset/easy_dataset.jsonlines                 cli.py:178\n",
      "           output directory: 2pl/easyx/                              cli.py:179\n",
      "[07:45:55] amortized: False                                      dataset.py:112\n",
      "[07:46:35] Vocab size: None                                      training.py:90\n",
      "[07:46:37] Training Model...                                         cli.py:209\n",
      "[07:46:37] args: {'device': 'cpu', 'num_items': 52766,          training.py:134\n",
      "           'num_subjects': 5230}                                               \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':    training.py:147\n",
      "           52766, 'num_subjects': 5230, 'priors': 'vague',                     \n",
      "           'dropout': 0.5, 'hidden': 100, 'vocab_size': None}                  \n",
      "torch.Size([8419626]) torch.Size([8419626])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┌───────┬────────────────┬────────────────┬────────┐\n",
      "│ Epoch │ Loss           │ Best Loss      │ New LR │\n",
      "├───────┼────────────────┼────────────────┼────────┤\n",
      "│ 1     │ 302960796.5000 │ 302960796.5000 │ 0.1000 │\n",
      "│ 101   │ 6981442.2893   │ 6981442.2893   │ 0.0990 │\n",
      "│ 201   │ 4409905.5084   │ 4409905.5084   │ 0.0980 │\n",
      "│ 301   │ 3646843.1492   │ 3641726.4314   │ 0.0970 │\n",
      "│ 401   │ 3287753.8529   │ 3287753.8529   │ 0.0961 │\n",
      "│ 501   │ 3088164.6088   │ 3084597.7199   │ 0.0951 │\n",
      "│ 601   │ 2967896.3653   │ 2965564.2104   │ 0.0942 │\n",
      "│ 701   │ 2885204.2970   │ 2881658.5183   │ 0.0932 │\n",
      "│ 801   │ 2828626.8317   │ 2826174.8159   │ 0.0923 │\n",
      "│ 901   │ 2786717.6446   │ 2784856.9798   │ 0.0914 │\n",
      "│ 1001  │ 2757321.3076   │ 2753589.6528   │ 0.0905 │\n",
      "│ 1101  │ 2731707.2041   │ 2731210.7344   │ 0.0896 │\n",
      "│ 1201  │ 2711477.0802   │ 2711477.0802   │ 0.0887 │\n",
      "│ 1301  │ 2697519.4377   │ 2697519.4377   │ 0.0878 │\n",
      "│ 1401  │ 2687250.6284   │ 2686548.4478   │ 0.0869 │\n",
      "│ 1501  │ 2677126.0759   │ 2677126.0759   │ 0.0861 │\n",
      "│ 1601  │ 2670560.4335   │ 2669098.4573   │ 0.0852 │\n",
      "│ 1701  │ 2663706.8694   │ 2663038.4492   │ 0.0844 │\n",
      "│ 1801  │ 2658282.8862   │ 2657768.7310   │ 0.0835 │\n",
      "│ 1901  │ 2653792.9681   │ 2653792.9681   │ 0.0827 │\n",
      "│ 2000  │ 2650917.3143   │ 2650297.4106   │ 0.0819 │\n",
      "└───────┴────────────────┴────────────────┴────────┘[07:54:49] Evaluating Model...                                       cli.py:216\n",
      "[07:54:53] Evaluation time: 552.8819530010223                        cli.py:244\n"
     ]
    }
   ],
   "source": [
    "!poetry run py-irt train-and-evaluate 2pl dataset/easy_dataset.jsonlines 2pl/easyx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:07:21] config: model_type='2pl' epochs=2000 priors=None          cli.py:176\n",
      "           initializers=[] dims=None lr=0.1 lr_decay=0.9999                    \n",
      "           dropout=0.5 hidden=100 vocab_size=None log_every=100                \n",
      "           seed=None deterministic=False                                       \n",
      "           data_path: dataset/hard_dataset.jsonlines                 cli.py:178\n",
      "           output directory: 2pl/hardx/                              cli.py:179\n",
      "[09:07:36] amortized: False                                      dataset.py:112\n",
      "[09:08:15] Vocab size: None                                      training.py:90\n",
      "[09:08:18] Training Model...                                         cli.py:209\n",
      "[09:08:18] args: {'device': 'cpu', 'num_items': 52766,          training.py:134\n",
      "           'num_subjects': 5230}                                               \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':    training.py:147\n",
      "           52766, 'num_subjects': 5230, 'priors': 'vague',                     \n",
      "           'dropout': 0.5, 'hidden': 100, 'vocab_size': None}                  \n",
      "torch.Size([8419626]) torch.Size([8419626])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┌───────┬────────────────┬────────────────┬────────┐\n",
      "│ Epoch │ Loss           │ Best Loss      │ New LR │\n",
      "├───────┼────────────────┼────────────────┼────────┤\n",
      "│ 1     │ 301390244.7500 │ 301390244.7500 │ 0.1000 │\n",
      "│ 101   │ 8266434.2793   │ 8266434.2793   │ 0.0990 │\n",
      "│ 201   │ 5620051.8258   │ 5620051.8258   │ 0.0980 │\n",
      "│ 301   │ 4816263.7195   │ 4813714.6388   │ 0.0970 │\n",
      "│ 401   │ 4444109.0101   │ 4433158.2487   │ 0.0961 │\n",
      "│ 501   │ 4233532.4849   │ 4233532.4849   │ 0.0951 │\n",
      "│ 601   │ 4115674.6724   │ 4115454.2489   │ 0.0942 │\n",
      "│ 701   │ 4034011.7837   │ 4032939.9922   │ 0.0932 │\n",
      "│ 801   │ 3972766.3181   │ 3972766.3181   │ 0.0923 │\n",
      "│ 901   │ 3941248.7478   │ 3936069.2620   │ 0.0914 │\n",
      "│ 1001  │ 3906130.9608   │ 3906130.9608   │ 0.0905 │\n",
      "│ 1101  │ 3884884.6887   │ 3881219.9955   │ 0.0896 │\n",
      "│ 1201  │ 3865901.9595   │ 3864145.5166   │ 0.0887 │\n",
      "│ 1301  │ 3848234.4234   │ 3848234.4234   │ 0.0878 │\n",
      "│ 1401  │ 3839199.7517   │ 3836808.2665   │ 0.0869 │\n",
      "│ 1501  │ 3828400.1494   │ 3827631.5312   │ 0.0861 │\n",
      "│ 1601  │ 3823803.3793   │ 3819101.7956   │ 0.0852 │\n",
      "│ 1701  │ 3815024.0968   │ 3814996.6208   │ 0.0844 │\n",
      "│ 1801  │ 3809689.7800   │ 3808438.7551   │ 0.0835 │\n",
      "│ 1901  │ 3805967.0772   │ 3804620.5121   │ 0.0827 │\n",
      "│ 2000  │ 3803685.9838   │ 3800328.3640   │ 0.0819 │\n",
      "└───────┴────────────────┴────────────────┴────────┘[09:16:40] Evaluating Model...                                       cli.py:216\n",
      "[09:16:44] Evaluation time: 562.9868953227997                        cli.py:244\n"
     ]
    }
   ],
   "source": [
    "!poetry run py-irt train-and-evaluate 2pl dataset/hard_dataset.jsonlines 2pl/hardx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in 'ability': 5230\n",
      "Number of items in 'diff': 52766\n",
      "Number of items in 'disc': 52766\n",
      "Number of items in 'item_ids': 52766\n",
      "Number of items in 'subject_ids': 5230\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('2pl/easyx/best_parameters.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the number of items in each dictionary\n",
    "num_ability_items = len(data[\"ability\"])\n",
    "num_diff_items = len(data[\"diff\"])\n",
    "num_disc_items = len(data[\"disc\"])\n",
    "num_item_ids = len(data[\"item_ids\"])\n",
    "num_subject_ids = len(data[\"subject_ids\"])\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Number of items in 'ability': {num_ability_items}\")\n",
    "print(f\"Number of items in 'diff': {num_diff_items}\")\n",
    "print(f\"Number of items in 'disc': {num_disc_items}\")\n",
    "print(f\"Number of items in 'item_ids': {num_item_ids}\")\n",
    "print(f\"Number of items in 'subject_ids': {num_subject_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in 'ability': 5230\n",
      "Number of items in 'diff': 52766\n",
      "Number of items in 'disc': 52766\n",
      "Number of items in 'item_ids': 52766\n",
      "Number of items in 'subject_ids': 5230\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('2pl/hardx/best_parameters.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the number of items in each dictionary\n",
    "num_ability_items = len(data[\"ability\"])\n",
    "num_diff_items = len(data[\"diff\"])\n",
    "num_disc_items = len(data[\"disc\"])\n",
    "num_item_ids = len(data[\"item_ids\"])\n",
    "num_subject_ids = len(data[\"subject_ids\"])\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Number of items in 'ability': {num_ability_items}\")\n",
    "print(f\"Number of items in 'diff': {num_diff_items}\")\n",
    "print(f\"Number of items in 'disc': {num_disc_items}\")\n",
    "print(f\"Number of items in 'item_ids': {num_item_ids}\")\n",
    "print(f\"Number of items in 'subject_ids': {num_subject_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output hasil\n",
    "\n",
    "### Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for est_level: 0.03363296658688851\n",
      "                                   md5  \\\n",
      "4257  bed8c515b04d9fa79d19d51a57890a28   \n",
      "4258  508330b4bc4513536aea7945d90909e2   \n",
      "4256  975edffb868ef4d1cde4b845022ea316   \n",
      "1475  b341e4cc6f0f6a59100910ed11a92f8d   \n",
      "2965  03c906bf1229a701a0471165242e1233   \n",
      "...                                ...   \n",
      "1386  fed59d3b700499c9207c4995d160cc9a   \n",
      "1381  8fdd4ffb45d79ef2e713a01abc9a08c3   \n",
      "1383  e01344b7d64159e7aa28b29d32ea6351   \n",
      "1382  bb843baa1332cdacd049f1e142c79045   \n",
      "1384  f880de58c2dcb3ea4e3d9dc39b096080   \n",
      "\n",
      "                                                  title  base_level  \\\n",
      "4257                  さいこ゛のたたかい /たひ゛のおわり [7Key Another]        11.0   \n",
      "4258                                            マジカル縦連打        11.0   \n",
      "4256                            Thrill Trigger EX2 YPER        11.0   \n",
      "1475                               Lieselotte [ANOTHER]        11.0   \n",
      "2965                                Pure Ruby [Another]        11.0   \n",
      "...                                                 ...         ...   \n",
      "1386                   the lost dedicated [life normal]        38.0   \n",
      "1381                                Dusk Recapture [AB]        38.0   \n",
      "1383  Nyan-Nyan Naughty Night [N,N,N′,N′-Tetramethyl...        38.0   \n",
      "1382                                    MESMERA [KOOKY]        38.0   \n",
      "1384                      saucy plume (KurisumasuFumen)        38.0   \n",
      "\n",
      "      prediction    ability  player_count  clear_count  est_level  \n",
      "4257    0.914496   0.953454        1236.0       1173.0  10.946203  \n",
      "4258    0.908785   4.697621        1205.0       1144.0  10.951914  \n",
      "4256    0.891855   0.472832        1245.0       1175.0  10.968845  \n",
      "1475    0.889777  13.756712         164.0        159.0  10.970922  \n",
      "2965    0.889685   8.044311           2.0          2.0  10.971014  \n",
      "...          ...        ...           ...          ...        ...  \n",
      "1386    0.829124  -2.275541          19.0         16.0  38.000000  \n",
      "1381    0.270609  -3.370217           2.0          1.0  38.000000  \n",
      "1383    0.203749  -2.772997           1.0          0.0  38.044556  \n",
      "1382    0.159601  -1.768321           1.0          0.0  38.088704  \n",
      "1384    0.073941  -2.553566           1.0          0.0  38.174364  \n",
      "\n",
      "[5194 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Membaca data chart asli sebagai referensi metadata\n",
    "with open('dataset/combined_dataset.json', 'r', encoding=\"utf-8\") as chart_file:\n",
    "    combined_data = json.load(chart_file)\n",
    "\n",
    "# Membaca parameter kemampuan pemain dari model 2PL\n",
    "best_parameters = []\n",
    "with open('2pl/easyx/best_parameters.json', 'r', encoding=\"utf-8\") as parameters_file:\n",
    "    for line in parameters_file:\n",
    "        params = json.loads(line)\n",
    "        abilities = params['ability']\n",
    "        # Mapping subject_id dengan kemampuan\n",
    "        for subject_id, ability in zip(params['subject_ids'].values(), abilities):\n",
    "            best_parameters.append({'subject_id': subject_id, 'ability': ability})\n",
    "\n",
    "# Membaca prediksi model dari file JSON Lines\n",
    "model_predictions = []\n",
    "with open('2pl/easyx/model_predictions.jsonlines', 'r', encoding=\"utf-8\") as predictions_file:\n",
    "    for line in predictions_file:\n",
    "        model_predictions.append(json.loads(line))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk pengolahan data\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "parameters_df = pd.DataFrame(best_parameters)\n",
    "predictions_df = pd.DataFrame(model_predictions)\n",
    "\n",
    "# Membersihkan data prediksi yang tidak valid\n",
    "predictions_df.dropna(subset=['prediction'], inplace=True)\n",
    "\n",
    "# Agregasi statistik performa per chart\n",
    "summary_df = predictions_df.groupby('subject_id').agg({\n",
    "    'response': lambda x: (x == 1).sum(),  # Hitung clear success\n",
    "    'prediction': ['mean', 'count']  # Rata-rata prediksi dan jumlah player\n",
    "}).reset_index()\n",
    "# Flatten multi-index column\n",
    "summary_df.columns = ['subject_id', 'clear', 'average_prediction', 'playercount']\n",
    "\n",
    "# Konversi tipe data untuk konsistensi\n",
    "summary_df['subject_id'] = summary_df['subject_id'].astype(str)\n",
    "\n",
    "# Gabungkan semua data menjadi DataFrame akhir\n",
    "result_df = pd.DataFrame({\n",
    "    'md5': combined_df['md5'],  # Identifier unik chart\n",
    "    'title': combined_df['title'],  # Judul chart\n",
    "    'base_level': combined_df['level'],  # Kesulitan dasar\n",
    "    'prediction': summary_df['average_prediction'],  # Probabilitas clear rata-rata\n",
    "    'ability': parameters_df['ability'],  # Parameter kemampuan dari model IRT\n",
    "    'player_count': summary_df['playercount'],  # Jumlah pemain yang mencoba\n",
    "    'clear_count': summary_df['clear']  # Jumlah clear sukses\n",
    "})\n",
    "# Bersihkan baris dengan data tidak lengkap\n",
    "result_df.dropna(inplace=True)\n",
    "\n",
    "def calculate_est_level(row):\n",
    "    ### Menghitung level estimasi berdasarkan distribusi prediksi per base level\n",
    "    # Ekstrak nilai dasar dan prediksi dari row\n",
    "    base_level = row['base_level']\n",
    "    prediction = row['prediction']\n",
    "    \n",
    "    # Hitung batas zona mid (percentil 33.33 dan 66.67) untuk base level yang sama\n",
    "    mid_zone_start = np.percentile(result_df[result_df['base_level'] == base_level]['prediction'], 33.33)\n",
    "    mid_zone_end = np.percentile(result_df[result_df['base_level'] == base_level]['prediction'], 66.67)\n",
    "\n",
    "    if prediction <= mid_zone_start:\n",
    "        return base_level + (mid_zone_start - prediction)  # Jika prediksi di bawah batas zona bawah\n",
    "    elif mid_zone_start < prediction <= mid_zone_end:\n",
    "        return base_level  # Jika prediksi berada di zona tengah\n",
    "    else:\n",
    "        return base_level - (prediction - mid_zone_end)  # Jika prediksi di atas batas zona atas\n",
    "\n",
    "# Terapkan fungsi ke setiap baris dan tambahkan kolom baru\n",
    "result_df['est_level'] = result_df.apply(calculate_est_level, axis=1)\n",
    "# Urutkan DataFrame berdasarkan level estimasi\n",
    "result_df.sort_values(by='est_level', ascending=True, inplace=True)\n",
    "\n",
    "# Menghitung Mean Absolute Error (MAE) antara level aktual dan prediksi\n",
    "mae_est_level = mean_absolute_error(result_df['base_level'], result_df['est_level'])\n",
    "# Menampilkan hasil perhitungan MAE\n",
    "print(\"MAE for est_level:\", mae_est_level)\n",
    "\n",
    "# Output file ke csv\n",
    "result_df.to_csv('output/easy_output_final.csv', index=False)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Membaca file CSV hasil pengolahan data final\n",
    "df = pd.read_csv('output/easy_output_final.csv')  # Memuat data hasil perhitungan ke DataFrame\n",
    "\n",
    "# Mengkonversi DataFrame ke format HTML menggunakan tabulate\n",
    "html_table = tabulate(df, headers='keys', tablefmt='html')  # Generate tabel HTML dasar\n",
    "\n",
    "# Membuat template HTML dengan styling CSS\n",
    "styled_html_table = f'''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Easy Clear Estimation</title>\n",
    "    <style>\n",
    "        table {{border-collapse: collapse; border: 2px solid black;}}  /* Garis tepi tebal untuk tabel */\n",
    "        th, td {{border: 1px solid black; padding: 10px;}}  /* Padding dan border untuk sel */\n",
    "        tr:nth-child(even) {{background-color: #ff9393;}}  /* Warna latar bergantian untuk baris */\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Easy Clear Estimation</h1>\n",
    "    {html_table}  <!-- Menyisipkan tabel yang telah digenerate -->\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Menyimpan hasil akhir ke file HTML\n",
    "with open('output/easy_output_final.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(styled_html_table)  # Tulis konten HTML ke file dengan encoding UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for est_level: 0.039402934422971686\n",
      "                                   md5  \\\n",
      "2965  03c906bf1229a701a0471165242e1233   \n",
      "2961  a23ea067c01254f860a55171dbee4f89   \n",
      "2971  8bdc1d1fe654d75136173211f124ad70   \n",
      "4257  bed8c515b04d9fa79d19d51a57890a28   \n",
      "4258  508330b4bc4513536aea7945d90909e2   \n",
      "...                                ...   \n",
      "1386  fed59d3b700499c9207c4995d160cc9a   \n",
      "1383  e01344b7d64159e7aa28b29d32ea6351   \n",
      "1381  8fdd4ffb45d79ef2e713a01abc9a08c3   \n",
      "1382  bb843baa1332cdacd049f1e142c79045   \n",
      "1384  f880de58c2dcb3ea4e3d9dc39b096080   \n",
      "\n",
      "                                                  title  base_level  \\\n",
      "2965                                Pure Ruby [Another]        11.0   \n",
      "2961                          Master of GENOCIDE [発狂入門]        11.0   \n",
      "2971                                 コスモワンダラー [SAETHER]        11.0   \n",
      "4257                  さいこ゛のたたかい /たひ゛のおわり [7Key Another]        11.0   \n",
      "4258                                            マジカル縦連打        11.0   \n",
      "...                                                 ...         ...   \n",
      "1386                   the lost dedicated [life normal]        38.0   \n",
      "1383  Nyan-Nyan Naughty Night [N,N,N′,N′-Tetramethyl...        38.0   \n",
      "1381                                Dusk Recapture [AB]        38.0   \n",
      "1382                                    MESMERA [KOOKY]        38.0   \n",
      "1384                      saucy plume (KurisumasuFumen)        38.0   \n",
      "\n",
      "      prediction   ability  player_count  clear_count  est_level  \n",
      "2965    0.896169  3.427751           2.0          2.0  10.834541  \n",
      "2961    0.879120  4.897824           3.0          3.0  10.851590  \n",
      "2971    0.859551  3.860892           1.0          1.0  10.871159  \n",
      "4257    0.784687 -0.243510        1236.0        985.0  10.946023  \n",
      "4258    0.783708  0.943088        1205.0        904.0  10.947003  \n",
      "...          ...       ...           ...          ...        ...  \n",
      "1386    0.770958 -3.269377          19.0         15.0  38.000000  \n",
      "1383    0.287263 -3.582881           1.0          0.0  38.000000  \n",
      "1381    0.286962 -3.954045           2.0          0.0  38.000200  \n",
      "1382    0.232156 -1.981480           1.0          0.0  38.055007  \n",
      "1384    0.089361 -3.558089           1.0          0.0  38.197801  \n",
      "\n",
      "[5194 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Membaca data chart asli sebagai referensi metadata\n",
    "with open('dataset/combined_dataset.json', 'r', encoding=\"utf-8\") as chart_file:\n",
    "    combined_data = json.load(chart_file)\n",
    "    \n",
    "# Membaca parameter kemampuan pemain dari model 2PL\n",
    "best_parameters = []\n",
    "with open('2pl/hardx/best_parameters.json', 'r', encoding=\"utf-8\") as parameters_file:\n",
    "    for line in parameters_file:\n",
    "        params = json.loads(line)\n",
    "        abilities = params['ability']\n",
    "        # Mapping subject_id dengan kemampuan\n",
    "        for subject_id, ability in zip(params['subject_ids'].values(), abilities):\n",
    "            best_parameters.append({'subject_id': subject_id, 'ability': ability})\n",
    "\n",
    "# Membaca prediksi model dari file JSON Lines            \n",
    "model_predictions = []\n",
    "with open('2pl/hardx/model_predictions.jsonlines', 'r', encoding=\"utf-8\") as predictions_file:\n",
    "    for line in predictions_file:\n",
    "        model_predictions.append(json.loads(line))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk pengolahan data\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "parameters_df = pd.DataFrame(best_parameters)\n",
    "predictions_df = pd.DataFrame(model_predictions)\n",
    "\n",
    "# Membersihkan data prediksi yang tidak valid\n",
    "predictions_df.dropna(subset=['prediction'], inplace=True)\n",
    "\n",
    "# Agregasi statistik performa per chart\n",
    "summary_df = predictions_df.groupby('subject_id').agg({\n",
    "    'response': lambda x: (x == 1).sum(),  # Hitung clear success\n",
    "    'prediction': ['mean', 'count']  # Rata-rata prediksi dan jumlah player\n",
    "}).reset_index()\n",
    "# Flatten multi-index column\n",
    "summary_df.columns = ['subject_id', 'clear', 'average_prediction', 'playercount']\n",
    "\n",
    "# Konversi tipe data untuk konsistensi\n",
    "summary_df['subject_id'] = summary_df['subject_id'].astype(str)\n",
    "\n",
    "# Gabungkan semua data menjadi DataFrame akhir\n",
    "result_df = pd.DataFrame({\n",
    "    'md5': combined_df['md5'],  # Identifier unik chart\n",
    "    'title': combined_df['title'],  # Judul chart\n",
    "    'base_level': combined_df['level'],  # Kesulitan dasar\n",
    "    'prediction': summary_df['average_prediction'],  # Probabilitas clear rata-rata\n",
    "    'ability': parameters_df['ability'],  # Parameter kemampuan dari model IRT\n",
    "    'player_count': summary_df['playercount'],  # Jumlah pemain yang mencoba\n",
    "    'clear_count': summary_df['clear']  # Jumlah clear sukses\n",
    "})\n",
    "# Bersihkan baris dengan data tidak lengkap\n",
    "result_df.dropna(inplace=True)\n",
    "\n",
    "def calculate_est_level(row):\n",
    "    ### Menghitung level estimasi berdasarkan distribusi prediksi per base level\n",
    "    # Ekstrak nilai dasar dan prediksi dari row\n",
    "    base_level = row['base_level']\n",
    "    prediction = row['prediction']\n",
    "    \n",
    "    # Hitung batas zona mid (percentil 33.33 dan 66.67) untuk base level yang sama\n",
    "    mid_zone_start = np.percentile(result_df[result_df['base_level'] == base_level]['prediction'], 33.33)\n",
    "    mid_zone_end = np.percentile(result_df[result_df['base_level'] == base_level]['prediction'], 66.67)\n",
    "\n",
    "    if prediction <= mid_zone_start:\n",
    "        return base_level + (mid_zone_start - prediction)  # Jika prediksi di bawah batas zona bawah\n",
    "    elif mid_zone_start < prediction <= mid_zone_end:\n",
    "        return base_level  # Jika prediksi berada di zona tengah\n",
    "    else:\n",
    "        return base_level - (prediction - mid_zone_end)  # Jika prediksi di atas batas zona atas\n",
    "\n",
    "# Terapkan fungsi ke setiap baris dan tambahkan kolom baru\n",
    "result_df['est_level'] = result_df.apply(calculate_est_level, axis=1)\n",
    "# Urutkan DataFrame berdasarkan level estimasi\n",
    "result_df.sort_values(by='est_level', ascending=True, inplace=True)\n",
    "\n",
    "# Menghitung Mean Absolute Error (MAE) antara level aktual dan prediksi\n",
    "mae_est_level = mean_absolute_error(result_df['base_level'], result_df['est_level'])\n",
    "# Menampilkan hasil perhitungan MAE\n",
    "print(\"MAE for est_level:\", mae_est_level)\n",
    "\n",
    "# Output file ke csv\n",
    "result_df.to_csv('output/hard_output_final.csv', index=False)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Membaca file CSV hasil pengolahan data final\n",
    "df = pd.read_csv('output/hard_output_final.csv')  # Memuat data hasil perhitungan ke DataFrame\n",
    "\n",
    "# Mengkonversi DataFrame ke format HTML menggunakan tabulate\n",
    "html_table = tabulate(df, headers='keys', tablefmt='html')  # Generate tabel HTML dasar\n",
    "\n",
    "# Membuat template HTML dengan styling CSS\n",
    "styled_html_table = f'''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Hard Clear Estimation</title>\n",
    "    <style>\n",
    "        table {{border-collapse: collapse; border: 2px solid black;}}  /* Garis tepi tebal untuk tabel */\n",
    "        th, td {{border: 1px solid black; padding: 10px;}}  /* Padding dan border untuk sel */\n",
    "        tr:nth-child(even) {{background-color: #ff9393;}}  /* Warna latar bergantian untuk baris */\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Hard Clear Estimation</h1>\n",
    "    {html_table}  <!-- Menyisipkan tabel yang telah digenerate -->\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Menyimpan hasil akhir ke file HTML\n",
    "with open('output/hard_output_final.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(styled_html_table)  # Tulis konten HTML ke file dengan encoding UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-irt-hOXkc2ls-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
